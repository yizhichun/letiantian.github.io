<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="乐天笔记, 樂天笔记, 编程" />





  <link rel="alternate" href="/atom.xml" title="樂天笔记" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/content/images/favicon.ico?v=5.1.1" />






<meta property="og:type" content="website">
<meta property="og:title" content="樂天笔记">
<meta property="og:url" content="http://www.letiantian.me/page/8/index.html">
<meta property="og:site_name" content="樂天笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="樂天笔记">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.letiantian.me/page/8/"/>





  <title>樂天笔记</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <img class='bg-image' src="/content/images/bg.jpg" />

  







  <script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=62535551";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>









  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">樂天笔记</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-topics">
          <a href="/topics" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-flask"></i> <br />
            
            专题
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.letiantian.me/2014-11-20-introduce-movielens-dataset/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Letian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/content/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="樂天笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014-11-20-introduce-movielens-dataset/" itemprop="url">MovieLens数据集介绍</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-11-20T15:19:05+08:00">
                November 20th 2014
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>2014-11-20</p>
<p><a href="http://grouplens.org/datasets/movielens/" target="_blank" rel="external">MovieLens数据集</a>保存了用户对电影的评分。基于这个数据集，我们可以测试一些推荐算法、评分预测算法。</p>
<h2 id="MovieLens-100k"><a href="#MovieLens-100k" class="headerlink" title="MovieLens 100k"></a>MovieLens 100k</h2><hr>
<p>该数据集记录了943个用户对1682部电影的共100,000个评分，每个用户至少对20部电影进行了评分。</p>
<p><strong>文件<code>u.info</code></strong>保存了该数据集的概要：</p>
<pre><code>943 users
1682 items
100000 ratings
</code></pre><p><strong>文件<code>u.item</code></strong>保存了item的信息，也就是电影的信息，共1682部电影，其id依次是1、2、……、1682。文件中每一行保存了一部电影的信息，格式如下：</p>
<pre><code>movie id | movie title | release date | video release date |
IMDb URL | unknown | Action | Adventure | Animation |
Children&#39;s | Comedy | Crime | Documentary | Drama | Fantasy |
Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |
Thriller | War | Western |
</code></pre><p>注意，最后19个字段保存的是该电影的类型，一个字段对应一个类型，值为0代表不属于该类型，值为1代表属于该类型，类型信息保存在文件<code>u.genre</code>中。</p>
<p>随便浏览了一下<code>u.item</code>文件，发现基本是20世纪90年代的电影。</p>
<p><strong>文件<code>u.genre</code></strong>保存了电影的类型信息。</p>
<p><strong>文件<code>u.user</code></strong>保存了用户的信息，共有943个用户，其id依次是1、2、……、943。文件中每一行保存了一个用户的信息，格式如下：</p>
<pre><code>user id | age | gender | occupation | zip code
</code></pre><p><strong>文件<code>u.occupation</code></strong>保存了用户职业的集合。</p>
<hr>
<p>下面介绍数据集的主要文件。</p>
<p><strong>文件<code>u.data</code></strong>保存了所有的评分记录，每一行是一个用户对一部电影的评分，共有100000条记录。当然，如果某用户没有对某电影评分，则不会包含在该文件中。评分的分值在1到5之间，就是1、2、3、5这5个评分。每一行格式如下：</p>
<pre><code>user id | item id | rating | timestamp
</code></pre><p>其中，<code>item id</code>就是电影的id，时间戳timestamp是评分时间。我转换了下时间戳，也是在20世纪90年代。</p>
<p><strong>文件<code>u1.base</code></strong>和<strong>文件<code>u1.test</code></strong>放在一起就是<strong>文件<code>u.data</code></strong>。将<code>u.data</code>按照<code>80%/20%</code>的比例分成<code>u1.base</code>和<code>u1.test</code>，可以将<code>u1.base</code>作为训练集，<code>u1.test</code>作为测试集。<code>u2</code>、<code>u3</code>、<code>u4</code>、<code>u5</code>系列文件和<code>u1</code>类似。<code>u1</code>、<code>u2</code>、<code>u3</code>、<code>u4</code>、<code>u5</code>的测试集是不相交的，它们可以用来做<strong>（5折交叉验证）5 fold cross validation</strong>。</p>
<p><strong>文件<code>ua.base</code></strong>和<strong>文件<code>ua.test</code></strong>也是由<code>u.data</code>拆分而来，在<code>ua.test</code>中包含了每个用户对10部电影的评分，从<code>u.data</code>去掉<code>ua.test</code>得到<code>ua.base</code>。<code>ub.base</code>和<code>ub.test</code>也使用了同样的生成方法。另外，<code>ua.test</code>和<code>ub.test</code>是不相交的。</p>
<h2 id="MovieLens-1M"><a href="#MovieLens-1M" class="headerlink" title="MovieLens 1M"></a>MovieLens 1M</h2><hr>
<p>该数据集保存的是6040个用户对3952部电影的1000209个评分记录。具体可以参考其<code>README</code>文件。</p>
<h2 id="MovieLens-10M"><a href="#MovieLens-10M" class="headerlink" title="MovieLens 10M"></a>MovieLens 10M</h2><hr>
<p>71567个用户，10681部电影，10000054条评分记录，同时多了个用户为电影设置的标签。具体可以阅读其中的<code>README.html</code>。</p>
<h2 id="Tag-Genome"><a href="#Tag-Genome" class="headerlink" title="Tag Genome"></a>Tag Genome</h2><hr>
<p>该数据集下有三个数据文件。</p>
<p><strong><code>movies.dat</code>：</strong>其每一行的格式是：</p>
<pre><code>&lt;MovieID&gt;&lt;Title&gt;&lt;MoviePopularity&gt;
</code></pre><p><code>MoviePopularity</code>是在MovieLens中对该电影的评分次数。</p>
<p><strong><code>tag.dat</code>：</strong>每一行的格式是：</p>
<pre><code>&lt;TagID&gt;&lt;Tag&gt;&lt;TagPopularity&gt;
</code></pre><p><code>&lt;TagPopularity&gt;</code>是使用该Tag的用户数，一个用户最多算1次。</p>
<p><strong><code>tag_relevance.dat</code>：</strong>：每一行的格式是：</p>
<pre><code>&lt;MovieID&gt;&lt;TagID&gt;&lt;Relevance&gt;
</code></pre><p><code>&lt;Relevance&gt;</code>的值在0和1之间，值越大，Tag与Movie的关联性越强。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.letiantian.me/2014-11-19-map-reduce-matrix-multiplication/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Letian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/content/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="樂天笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014-11-19-map-reduce-matrix-multiplication/" itemprop="url">如何使用MapRedue实现矩阵乘法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-11-19T21:50:10+08:00">
                November 19th 2014
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>2014-11-19</p>
<p>本文介绍两种常用的矩阵乘法的实现。</p>
<h2 id="关于矩阵乘法"><a href="#关于矩阵乘法" class="headerlink" title="关于矩阵乘法"></a>关于矩阵乘法</h2><hr>
<p>设矩阵A大小为<code>m*p</code>，矩阵B大小为<code>p*n</code>，<code>C=A*B</code>，C的大小为<code>m*n</code>。矩阵中每个元素的行号和列号均从<strong>1</strong>开始，矩阵C可以通过下面的公式计算得到。</p>
<p>[latex]<br>C_{i,j}=\sum_{k=1}^{p}A_{i,k}*B_{k,j}<br>[/latex]</p>
<h2 id="实现方案1"><a href="#实现方案1" class="headerlink" title="实现方案1"></a>实现方案1</h2><hr>
<p>在文件中每一行存储矩阵中的一个元素，每一行格式如下：</p>
<pre><code class="no-highlight">所属矩阵#行号#列号#值
</code></pre>
<p>例如，若矩阵A是：</p>
<pre><code>2 3
4 1
1 0
</code></pre><p>矩阵B是：</p>
<pre><code>2
7
</code></pre><p>A在数据文件中对应的文件内容是：</p>
<pre><code class="no-highlight">A#1#1#2
A#1#2#3
A#2#1#4
A#2#2#1
A#3#1#1
A#3#2#0
</code></pre>
<p>B在数据文件中对应的文件内容是：</p>
<pre><code class="no-highlight">B#1#1#2
B#2#1#7
</code></pre>
<p>上面是Map Task的输入，对于每一行输入Map Task的输出中key和value的格式是：</p>
<pre><code class="no-highlight">行号#(1...n)  -&gt; 所属矩阵#列号#对应的值
</code></pre>
<p>对于Map Task，每一行输入，有n个输出。</p>
<p>n为1，故对矩阵A，Map的输出是：</p>
<pre><code class="no-highlight">1#1 -&gt; A#1#2
1#1 -&gt; A#2#3
2#1 -&gt; A#1#4
2#1 -&gt; A#2#1
3#1 -&gt; A#1#1
3#1 -&gt; A#2#0
</code></pre>
<p>矩阵B的文件格式和A相同，对于每一行输入Map Task的输出中key和value的格式是：</p>
<pre><code class="no-highlight">(1...m)#列号  -&gt; 所属矩阵#行号#对应的值
</code></pre>
<p>m为3。故对矩阵B，对于每一行输入，Map Task的输出中key和value的格式是：</p>
<pre><code class="no-highlight">1#1 -&gt; B#1#2
1#1 -&gt; B#2#7
2#1 -&gt; B#1#2
2#1 -&gt; B#2#7
3#1 -&gt; B#1#2
3#1 -&gt; B#2#7
</code></pre>
<p>在Reduce过程中输入的相同的键的值将放在一起，例如对于键<code>1#1</code>，Reduce的输入中，values为：</p>
<pre><code class="no-highlight">A#1#2，A#2#3，B#1#2，B#2#7
</code></pre>
<p>构造两个向量（也就是数组）a和b，a[1]=2，a[2]=3，b[1]=2，b[2]=7，将a和b点乘，得到<code>2*2+3*7=25</code>，故C[1,1] = 25。<br> Reduce的输出中key和value的格式是：</p>
<pre><code class="no-highlight">行号#列号 -&gt; 结果
</code></pre>
<p>比如：</p>
<pre><code class="no-highlight">1#1 25
</code></pre>
<h2 id="实现方案2"><a href="#实现方案2" class="headerlink" title="实现方案2"></a>实现方案2</h2><hr>
<p>矩阵C在[i,j]处元素的值，其实就是矩阵A第i行、矩阵B第j列的点乘结果，所以可以让Map的输入的每个数据就是矩阵的一行或者一列。</p>
<p>对于矩阵A，数据文件中每行存储矩阵的一行，每行格式如下：</p>
<pre><code class="no-highlight">A#行号#这一行的数据
</code></pre>
<p>对于矩阵B，数据文件中每行存储矩阵的一列，每行格式如下：</p>
<pre><code class="no-highlight">B#列号#这一列的数据
</code></pre>
<p>于是可以得到数据文件：</p>
<pre><code class="no-highlight">A#1#2,3
A#2#4,1
A#3#1,0
B#1#2,7
</code></pre>
<p>以上的Map Task的输入。<br>对于矩阵A，每一行数据转换为：</p>
<pre><code class="no-highlight">行号#(1...n)  -&gt; 所属矩阵#这一行的值
</code></pre>
<p>对于矩阵B，每一列数据转换为：</p>
<pre><code class="no-highlight">(1...m)#列号  -&gt; 所属矩阵#这一列的值
</code></pre>
<p>所以Map Task的输出是：</p>
<pre><code class="no-highlight">1#1  A#2,3
2#1  A#4,1
3#1  A#1,0
1#1  B#2,7
2#1  B#2,7
3#1  B#2,7
</code></pre>
<p>Map Task的输出将作为Reduce Task的输入。在Reduce过程中输入的相同的键的值将放在一起，例如对于键<code>1#1</code>，Reduce的输入中，values为：</p>
<pre><code class="no-highlight">&quot;A#2,3&quot;, &quot;B#2,7&quot;
</code></pre>
<p>将向量<code>2,3</code>与向量<code>2,7</code>点乘结果为25，所有C[1,1]=25。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><hr>
<p>矩阵乘法还有其他的MapReduce实现思路，例如分块计算，这里暂且不做介绍了。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><hr>
<p>Hadoop大数据处理 刘军 著  第9章</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.letiantian.me/2014-11-19-hadoop-kmeans/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Letian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/content/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="樂天笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014-11-19-hadoop-kmeans/" itemprop="url">Hadoop 2.4 实现Kmeans聚类算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-11-19T11:40:24+08:00">
                November 19th 2014
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>2014-11-19</p>
<p>先说基本思路，再说实现。</p>
<h2 id="如何用MapReduce的思路实现Kmeans"><a href="#如何用MapReduce的思路实现Kmeans" class="headerlink" title="如何用MapReduce的思路实现Kmeans"></a>如何用MapReduce的思路实现Kmeans</h2><hr>
<p>这个思路参考自下面这篇论文：</p>
<blockquote>
<p>Zhao W, Ma H, He Q. Parallel k-means clustering based on mapreduce[M]//Cloud Computing. Springer Berlin Heidelberg, 2009: 674-679.</p>
</blockquote>
<p>对于有n个对象的数据集，如果要聚成k类，Kmeans的基本思路是：</p>
<p><strong>1、</strong>首先从这n个对象中随机选择k个对象作为初始的k个簇的中心（就叫做“簇心”吧）；</p>
<p><strong>2、</strong>然后将其余的对象分到最近的簇心，如此k个簇就出来了；</p>
<p><strong>3、</strong>之后对每个簇，求新的簇心（基本方法是将属于该簇的所有点放在一起求平均值）；</p>
<p><strong>4、</strong>重复2、3步（一轮2、3步，可以叫做一次<code>迭代</code>），直到所有的簇心基本不再变化，或者达到指定的重复次数；</p>
<p><strong>5、</strong>和第2步相同。</p>
<p>从上面的步骤中，可以看出在每次迭代中，计算距离这一操作需要执行<code>n*k</code>次；很容易看出，对一个对象求最近簇心并不受对另外一个对象求最近簇心的影响，所以这一步是可以并行的。由于每次迭代要用到的簇心是上一次迭代的结果，所以些迭代是串行的关系。</p>
<p>基于上面的分析，现在可以考虑如何实现kmeans的MapReduce版本了，下面是一次迭代中每个Task的伪代码实现。</p>
<h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><p>Map Task的输入数据以SequenceFile的形式存储在HDFS中，这种文件以<key, value="">这种键值对的形式存储每个样本。key是这个样本的存储位置在该数据文件中的偏移量（这个key没什么作用），value是字符串，是这个样本的内容。SequenceFile类型的文件是可拆分的，这意味着这些样本可以拆开并映射到多个Map Task中。每个个Map Task获取当前的簇心，计算每个样本最近的簇心。Map Task的输出中，值是一个样本，键是该样本对应的簇心的标号。</key,></p>
<pre><code class="no-highlight">**Input:** Global variable centers, the offset key, the sample value
**Output:** &lt;key’, value’&gt; pair, where the key’ is the index of the closest  center point and value’ is a string comprise of sample information

1. Construct the sample instance from value;
2. minDis = Double.MAX VALUE ;
3. index = -1;
4. For i=0 to centers.length do
    dis= ComputeDist(instance, centers[i]);
    If dis &lt; minDis {
        minDis = dis;
        index = i;
    }
5. End For
6. Take index as key’;
7. Construct value’ as a string comprise of the values of different dimensions;
8. output &lt; key , value &gt; pair;
9. End
</code></pre>
<h3 id="Combine"><a href="#Combine" class="headerlink" title="Combine"></a>Combine</h3><p>Combine Task是一个小型的Reduce Task，其输入是Map Task的输出。Combine Task的输出中，键还是簇心的标号，值由两部分组成，一个是同属于该簇心的所有样本的和，另外一个是这些样本的个数。</p>
<pre><code class="no-highlight">**Input:** key is the index of the cluster, V is the list of the samples assigned to the same cluster
**Output:** &lt; key , value &gt; pair, where the key’ is the index of the cluster, value’ is a string comprised of sum of the samples in the same cluster and the sample number

1. Initialize one array to record the sum of value of each dimensions of the samples contained in the same cluster, i.e. the samples in the list V ;
2. Initialize a counter num as 0 to record the sum of sample number in the same cluster;
3. while(V.hasNext()){
        Construct the sample instance from V.next();
        Add the values of different dimensions of instance to the array
        num++;
4. }
5. Take key as key’;
6. Construct value’ as a string comprised of the sum values of different dimensions and num;
7. output &lt; key , value &gt; pair;
8. End
</code></pre>
<h3 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h3><pre><code class="no-highlight">**Input: **key is the index of the cluster, V is the list of the partial sums from different host
**Output:** &lt; key , value &gt; pair, where the key’ is the index of the cluster, value’ is a string repre-
senting the new center
1. Initialize one array record the sum of value of each dimensions of the samples contained in the
same cluster, e.g. the samples in the list V ;
2. Initialize a counter NUM as 0 to record the sum of sample number in the same cluster;
3. while(V.hasNext()){
          Construct the sample instance from V.next();
          Add the values of different dimensions of instance to the array
          NUM += num;
4. }
5. Divide the entries of the array by NUM to get the new center’s coordinates; // 求簇心
6. Take key as key’;
7. Construct value’ as a string comprise of the center ’s coordinates;
8. output &lt; key , value &gt; pair;
9. End
</code></pre>
<h2 id="编程实现Kmeans"><a href="#编程实现Kmeans" class="headerlink" title="编程实现Kmeans"></a>编程实现Kmeans</h2><hr>
<p><a href="http://blog.csdn.net/fansy1990/article/details/8028546" target="_blank" rel="external">Hadoop k-means 算法实现</a>中贴出了kmeans实现的源码，原理和上面讲述的相同。</p>
<p>不过有一些值得注意的地方。</p>
<p>Kmeans包含了很多了顺序执行的MapReduce job，每次MapReduce job会产生新的簇心的信息并保存到HDFS中，这些新的簇心通过<code>org.apache.hadoop.filecache.DistributedCache</code>共享到下一次MapReduce job的Map Task中。</p>
<p>关于<code>org.apache.hadoop.filecache.DistributedCache</code>，可以参考：<br><a href="http://qindongliang.iteye.com/blog/2038108" target="_blank" rel="external">如何使用Hadoop的DistributedCache</a> 、<a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-distributedcache-details/" target="_blank" rel="external">Hadoop DistributedCache详解</a>，不过该类（至少）在Hadoop 2.4中已经被弃用了，新的API可以参考stackoverflow的这个提问：<a href="http://stackoverflow.com/questions/21239722/hadoop-distributedcache-is-deprecated-what-is-the-preferred-api" target="_blank" rel="external">Hadoop DistributedCache is deprecated - what is the preferred API?</a>。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.letiantian.me/2014-11-18-hadoop-2-4-hdfs-read-text-file/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Letian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/content/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="樂天笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014-11-18-hadoop-2-4-hdfs-read-text-file/" itemprop="url">Hadoop 2.4 在HDFS中读取文本文件的内容</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-11-18T21:13:56+08:00">
                November 18th 2014
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>2014-11-18</p>
<p>Hadoop版本是2.4.1，搭建于Linux Mint 16 之上。</p>
<p>在我的Desktop上，有文件<code>t1.txt</code>，内容如下：</p>
<pre><code>Sign up for GitHub. By clicking &quot;Sign up for GitHub&quot;, you agree to our terms of service and privacy policy. We will send you account related emails occasionally
</code></pre><p>在HDFS中的<code>/input/t1.txt</code>内容与上面的相同。</p>
<h2 id="创建项目并引入包"><a href="#创建项目并引入包" class="headerlink" title="创建项目并引入包"></a>创建项目并引入包</h2><hr>
<p>在eclipse中创建项目<code>LearnHDFS</code>，引入<code>hadoop-2.4.1/share/hadoop/common/hadoop-common-2.4.1.jar</code>、<code>hadoop-2.4.1/share/hadoop/hdfs/hadoop-hdfs-2.4.1.jar</code>以及<code>hadoop-2.4.1/share/hadoop/common/lib/</code>目录下的所有jar包。</p>
<h2 id="配置log4j"><a href="#配置log4j" class="headerlink" title="配置log4j"></a>配置log4j</h2><hr>
<p>在<code>LearnHDFS</code>项目中添加文件<code>log4j.properties</code>，内容如下：</p>
<pre><code>log4j.rootLogger=INFO, stdout  
log4j.appender.stdout=org.apache.log4j.ConsoleAppender  
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  
log4j.appender.logfile=org.apache.log4j.FileAppender  
log4j.appender.logfile.File=target/spring.log  
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
</code></pre><h2 id="读取本地文件"><a href="#读取本地文件" class="headerlink" title="读取本地文件"></a>读取本地文件</h2><hr>
<p>在<code>LearnHDFS</code>项目中添加文件<code>ReadLocalFile.java</code>，内容如下：</p>
<pre><code>import org.apache.hadoop.conf.*;
import org.apache.hadoop.fs.*;
import java.io.*;

public class ReadLocalFile {

    public static void main(String[] args) {
        try {
            Configuration conf = new Configuration();
            FileSystem fs = FileSystem.get(conf);
            Path file = new Path(&quot;file:///home/sunlt/Desktop/t1.txt&quot;);
            FSDataInputStream getIt = fs.open(file);
            BufferedReader d = new BufferedReader(new InputStreamReader(getIt));
            String s = &quot;&quot;;
            while ((s = d.readLine()) != null) {
                System.out.println(s);
            }
            d.close();
            fs.close();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

}
</code></pre><p>直接用eclipse运行<code>ReadLocalFile.java</code>，输出结果是：</p>
<pre><code>Sign up for GitHub. By clicking &quot;Sign up for GitHub&quot;, you agree to our terms of service and privacy policy. We will send you account related emails occasionally
</code></pre><h2 id="读取HDFS中的文件"><a href="#读取HDFS中的文件" class="headerlink" title="读取HDFS中的文件"></a>读取HDFS中的文件</h2><hr>
<p>在<code>LearnHDFS</code>项目中添加文件<code>ReadHDFSFile.java</code>，内容如下：</p>
<pre><code>import org.apache.hadoop.conf.*;
import org.apache.hadoop.fs.*;

import java.io.*;
import java.net.URI;

public class ReadHDFSFile {

    public static void main(String[] args) {
        try {
            Configuration conf = new Configuration();
            FileSystem fs = FileSystem.get(new URI(&quot;hdfs://localhost:9000&quot;), conf);
            Path file = new Path(&quot;/input/t1.txt&quot;);
            FSDataInputStream getIt = fs.open(file);
            BufferedReader d = new BufferedReader(new InputStreamReader(getIt));
            String s = &quot;&quot;;
            while ((s = d.readLine()) != null) {
                System.out.println(s);
            }
            d.close();
            fs.close();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

}
</code></pre><p>由于Hadop的core-site.xml中设置了：</p>
<pre><code>&lt;property&gt;
  &lt;name&gt;fs.default.name&lt;/name&gt;
    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
&lt;/property&gt;
</code></pre><p>所以<code>FileSystem.get()</code>方法的第一个参数设置为了<code>new URI(&quot;hdfs://localhost:9000&quot;)</code>。<br>直接用eclipse运行<code>ReadHDFSFile.java</code>，输出结果是：</p>
<pre><code>Sign up for GitHub. By clicking &quot;Sign up for GitHub&quot;, you agree to our terms of service and privacy policy. We will send you account related emails occasionally
</code></pre><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><hr>
<p><strong>1、</strong>如果只引入<code>hadoop-2.4.1/share/hadoop/common/hadoop-common-2.4.1.jar</code>和<code>hadoop-2.4.1/share/hadoop/hdfs/hadoop-hdfs-2.4.1.jar</code>，在eclipse中编写代码时并不会提示错误，但是当直接在eclipse下运行程序时，会因为找不到某些类而报错，比如：</p>
<pre><code>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/commons/logging/LogFactory
</code></pre><p><strong>2、</strong>如果没有设置log4j，在运行时会有警告：</p>
<pre><code>log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
</code></pre><p>所以最好添加文件<code>log4j.properties</code>，并做一些设置。</p>
<h2 id="关于HDFS的文件操作"><a href="#关于HDFS的文件操作" class="headerlink" title="关于HDFS的文件操作"></a>关于HDFS的文件操作</h2><hr>
<p>文件操作包括读写文件、创建目录、删除文件和目录、查看文件状态等操作，这些在Tom White的<strong>《Hadoop权威指南 第2版》第3章 Hadoop分布式文件系统</strong>中有较为详细的介绍。</p>
<p>下面两个博客中有总结好的代码：</p>
<p><a href="http://blog.csdn.net/hadoop_/article/details/9357905" target="_blank" rel="external">Hadoop HDFS文件操作 Java实现类</a>  </p>
<p><a href="http://appcrawler.com/wordpress/2013/05/30/querying-hadoop-from-tomcat/" target="_blank" rel="external">Querying Hadoop from Tomcat</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.letiantian.me/2014-11-17-hadoop-2-4-word-count-and-get-max-occurrence-number/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Letian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/content/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="樂天笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014-11-17-hadoop-2-4-word-count-and-get-max-occurrence-number/" itemprop="url">Hadoop 2.4 单词计数并获取最大词频</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-11-17T22:32:58+08:00">
                November 17th 2014
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>2014-11-17</p>
<p>这里的“词频”是指单词出现的频数，也就是次数。</p>
<p>在<a href="/2014-11-17-hadoop-2-4-word-count/">Hadoop 2.4 实现单词计数</a>中已经论述了如何进行单词计数，其处理的结果保存在HDFS中的<code>/output</code>目录下，其中有一文件<code>_SUCCESS</code>是空文件（因为是空文件，所以可以忽略），表示这个JOB成功执行了。  另外一个文件是<code>part-r-00000</code>，<code>r</code>代表着这个文件是reduce的结果。</p>
<p>现在对<code>/output</code>中的文件进行处理，获取最大词频。创建java文件<code>MaxNum.java</code>，内容如下：</p>
<pre><code class="java">import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class MaxNum {



    public static class MaxNumMapper extends
            Mapper&lt;Object, Text, IntWritable, IntWritable&gt; {

        private final static IntWritable onlyKey = new IntWritable(1);

        public void map(Object key, Text value, Context context)
                throws IOException, InterruptedException {
            String numStr = value.toString().split(&quot;\t&quot;)[1];
            context.write(onlyKey, new IntWritable(Integer.parseInt(numStr)));
        }
    }

    public static class MaxNumReducer extends
            Reducer&lt;IntWritable, IntWritable, Text, IntWritable&gt; {

        private final static Text onlyKey = new Text(&quot;max&quot;);
        private IntWritable result = new IntWritable();

        public void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values,
                Context context) throws IOException, InterruptedException {
            int max_num = 0;
            for (IntWritable val : values) {
                if ( max_num &lt; val.get()) {
                    max_num = val.get();
                }
            }
            result.set(max_num);
            context.write(onlyKey, result);
        }
    }

    public static void main(String[] args) throws Exception {

        Configuration conf2 = new Configuration();
        Job job2 = Job.getInstance(conf2, &quot;get max number&quot;);

        job2.setJarByClass(WordCountAndMaxNum.class);

        job2.setMapperClass(MaxNumMapper.class);
        job2.setMapOutputKeyClass(IntWritable.class);
        job2.setMapOutputValueClass(IntWritable.class);    


 //     job2.setCombinerClass(MaxNumReducer.class);
        job2.setReducerClass(MaxNumReducer.class);
        job2.setOutputKeyClass(Text.class);
        job2.setOutputValueClass(IntWritable.class);

        FileInputFormat.setInputPaths(job2, &quot;/output&quot;);
        FileOutputFormat.setOutputPath(job2, new Path(&quot;/output2&quot;));

        job2.waitForCompletion(true);

    }
}
</code></pre>
<p>注意，在<code>main()</code>函数中<code>job2.setCombinerClass(MaxNumReducer.class);</code>被注释掉了，如果不注释掉，在运行时会产生这样一个错误：</p>
<pre><code>Error: java.io.IOException: wrong key class: class org.apache.hadoop.io.Text is not class org.apache.hadoop.io.IntWritable
</code></pre><p>原因是这样的。Combiner过程发生在Map和Reduce之间，它是一个微型的Reduce（一个Combiner Task处理的数据量较小）。在设置Combiner后，意味着这整个JOB有两次Reduce，第一次是Combiner TASK调用MaxNumReducer类，输出的键值类型是<code>&lt;Text, IntWritable&gt;</code>，该输出会作为第二次Reduce的输入；第二次是Reduce TASK调用MaxNumReducer类，要求输入的键值类型为<code>IntWritable, IntWritable</code>，由此便产生了类型的冲突。</p>
<p>如果一定要加上Combiner，有两个方案：<br>1、修改<code>MaxNumReducer</code>类；<br>2、再添加一个继承了Reducer的类供Combiner单独使用。</p>
<p>下面我们将单词计数和获取最大词频整合在一起，创建<code>WordCountAndMaxNum.java</code>，内容如下：</p>
<pre><code class="java">import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCountAndMaxNum {

    public static class TokenizerMapper extends
            Mapper&lt;Object, Text, Text, IntWritable&gt; {

        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(Object key, Text value, Context context)
                throws IOException, InterruptedException {
            StringTokenizer itr = new StringTokenizer(value.toString());
            while (itr.hasMoreTokens()) {
                word.set(itr.nextToken());
                context.write(word, one);
            }
        }
    }

    public static class IntSumReducer extends
            Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable&lt;IntWritable&gt; values,
                Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }


    public static class MaxNumMapper extends
            Mapper&lt;Object, Text, IntWritable, IntWritable&gt; {

        private final static IntWritable onlyKey = new IntWritable(1);

        public void map(Object key, Text value, Context context)
                throws IOException, InterruptedException {
            String numStr = value.toString().split(&quot;\t&quot;)[1];
            context.write(onlyKey, new IntWritable(Integer.parseInt(numStr)));
        }
    }

    public static class MaxNumReducer extends
            Reducer&lt;IntWritable, IntWritable, Text, IntWritable&gt; {

        private final static Text onlyKey = new Text(&quot;max&quot;);
        private IntWritable result = new IntWritable();

        public void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values,
                Context context) throws IOException, InterruptedException {
            int max_num = 0;
            for (IntWritable val : values) {
                if ( max_num &lt; val.get()) {
                    max_num = val.get();
                }
            }
            result.set(max_num);
            context.write(onlyKey, result);
        }
    }

    public static void main(String[] args) throws Exception {

        Configuration conf1 = new Configuration();
        Job job1 = Job.getInstance(conf1, &quot;word count&quot;);

        job1.setJarByClass(WordCountAndMaxNum.class);

        job1.setMapperClass(TokenizerMapper.class);
        job1.setMapOutputKeyClass(Text.class);  //!
        job1.setMapOutputValueClass(IntWritable.class); //!

        job1.setCombinerClass(IntSumReducer.class);
        job1.setReducerClass(IntSumReducer.class);
        job1.setOutputKeyClass(Text.class);
        job1.setOutputValueClass(IntWritable.class);

        FileInputFormat.setInputPaths(job1, &quot;/input&quot;);
        FileOutputFormat.setOutputPath(job1, new Path(&quot;/output&quot;));

        job1.waitForCompletion(true);

        // --

        Configuration conf2 = new Configuration();
        Job job2 = Job.getInstance(conf2, &quot;get max number&quot;);

        job2.setJarByClass(WordCountAndMaxNum.class);

        job2.setMapperClass(MaxNumMapper.class);
        job2.setMapOutputKeyClass(IntWritable.class);
        job2.setMapOutputValueClass(IntWritable.class);    


//        job2.setCombinerClass(MaxNumReducer.class);
        job2.setReducerClass(MaxNumReducer.class);
        job2.setOutputKeyClass(Text.class);
        job2.setOutputValueClass(IntWritable.class);

        FileInputFormat.setInputPaths(job2, &quot;/output&quot;);
        FileOutputFormat.setOutputPath(job2, new Path(&quot;/output2&quot;));

        job2.waitForCompletion(true);

    }
}
</code></pre>
<p>仍然处理<a href="/2014-11-17-hadoop-2-4-word-count/">Hadoop 2.4 实现单词计数</a>中使用的文本，结果如下：</p>
<pre><code>zsh &gt;&gt; $HADOOP_PREFIX/bin/hadoop fs -cat /output2/part-r-00000                 
max    3
</code></pre><hr>
<p><strong>我在编码过程中遇到过这样一个问题：</strong></p>
<pre><code>java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.Text, received org.apache.hadoop.io.LongWritable
</code></pre><p>可以在<a href="http://stackoverflow.com/questions/17262188/type-mismatch-in-key-from-map-expected-org-apache-hadoop-io-text-recieved-org" target="_blank" rel="external">Type mismatch in key from map: expected org.apache.hadoop.io.Text, recieved org.apache.hadoop.io.LongWritable</a>找到答案。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.letiantian.me/2014-11-17-hadoop-2-4-word-count/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Letian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/content/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="樂天笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014-11-17-hadoop-2-4-word-count/" itemprop="url">Hadoop 2.4 实现单词计数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-11-17T16:53:57+08:00">
                November 17th 2014
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>2014-11-17</p>
<p>关于hadoop2.4的配置请参考<a href="/2014-10-16-hadoop-2-4-1-stand-alone-install-and-config/">hadoop2.4.1单机安装和配置</a>。</p>
<p>现在有两个文本文件<code>t1.txt</code>和<code>t2.txt</code>，内容分别是：</p>
<p>t1.txt：</p>
<pre><code class="no">Sign up for GitHub. By clicking &quot;Sign up for GitHub&quot;, you agree to our terms of service and privacy policy. We will send you account related emails occasionally
</code></pre>
<p>t2.txt：</p>
<pre><code>and and  you
</code></pre><p>问题1是，如何对t1.txt中的单词进行计数。</p>
<p>问题2是，如何对t1.txt和t2.txt中的单词进行计数。</p>
<h2 id="创建目录并将文本文件放入HDFS"><a href="#创建目录并将文本文件放入HDFS" class="headerlink" title="创建目录并将文本文件放入HDFS"></a>创建目录并将文本文件放入HDFS</h2><hr>
<pre><code>zsh &gt;&gt; $HADOOP_PREFIX/bin/hadoop fs -mkdir /input/
zsh &gt;&gt; $HADOOP_PREFIX/bin/hadoop fs -put t1.txt /input
zsh &gt;&gt; $HADOOP_PREFIX/bin/hadoop fs -put t2.txt /input
</code></pre><h2 id="对t1-txt中的单词进行计数"><a href="#对t1-txt中的单词进行计数" class="headerlink" title="对t1.txt中的单词进行计数"></a>对t1.txt中的单词进行计数</h2><hr>
<p>打开eclipse，创建项目WordCount，导入<code>hadoop-2.4.1/share/hadoop/common/hadoop-common-2.4.1.jar</code>和<code>hadoop-2.4.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.4.1.jar</code>，创建WordCount.java，将hadoop自带的wordcount源码粘贴进去并略做修改：</p>
<pre><code class="java">import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

    public static class TokenizerMapper extends
            Mapper&lt;Object, Text, Text, IntWritable&gt; {

        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(Object key, Text value, Context context)
                throws IOException, InterruptedException {
            StringTokenizer itr = new StringTokenizer(value.toString());
            while (itr.hasMoreTokens()) {
                word.set(itr.nextToken());
                context.write(word, one);
            }
        }
    }

    public static class IntSumReducer extends
            Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable&lt;IntWritable&gt; values,
                Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static void main(String[] args) throws Exception {
        System.out.println(&quot;start...&quot;);
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, &quot;word count&quot;);
        job.setJarByClass(WordCount.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(&quot;/input/t1.txt&quot;));
        FileOutputFormat.setOutputPath(job, new Path(&quot;/output&quot;));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
</code></pre>
<p>下面解释一下这段代码。<br>TokenizerMapper类继承自Mapper<object, text,="" intwritable="">，其原型如下：</object,></p>
<pre><code>@InterfaceAudience.Public
@InterfaceStability.Stable
public class Mapper&lt;KEYIN,VALUEIN,KEYOUT,VALUEOUT&gt;
extends Object
</code></pre><p><code>KEYIN</code>是map的输入数据的键的类型，<code>VALUEIN</code>是map的输入数据的值的类型，<code>KEYOUT</code>是map处理后输出的结果中键的类型，<code>VALUEOUT</code>是map处理后输出的结果中值的类型。</p>
<p>TokenizerMapper类的map函数中使用了StringTokenizer类，其根据空格、换行符等对一段文本进行拆分，功能比较简单（也不够实用），下面是一个例子：</p>
<pre><code class="java">import java.util.StringTokenizer;

public class Test {
    public static void main(String[] args) {
        StringTokenizer itr = new StringTokenizer(&quot;hello world! hi\nhadoop&quot;);
        while (itr.hasMoreTokens()) {
            System.out.println(itr.nextToken());
        }
    }
}
</code></pre>
<p>运行结果如下：</p>
<pre><code>hello
world!
hi
hadoop
</code></pre><p>IntSumReducer类以及main()函数就不介绍了。  有一点要注意，在main()函数中，设置了输入的文件是<code>/input/t1.txt</code>，MapReduce结果放入<code>/output</code>目录中。</p>
<p>将该项目导出为<code>WordCount.jar</code>包后，执行：</p>
<pre><code>$HADOOP_PREFIX/bin/hadoop jar WordCount.jar WordCount
</code></pre><p>运行完毕，查看reduce后的结果：</p>
<pre><code>zsh &gt;&gt; $HADOOP_PREFIX/bin/hadoop fs -cat /output/part-r-00000
&quot;Sign    1
By    1
GitHub&quot;,    1
GitHub.    1
Sign    1
We    1
account    1
agree    1
and    1
clicking    1
emails    1
for    2
occasionally    1
of    1
our    1
policy.    1
privacy    1
related    1
send    1
service    1
terms    1
to    1
up    2
will    1
you    2
</code></pre><p>结果正确。标点符号的混入以及单词大小写的问题可以根据需要完善一下。</p>
<h2 id="对t1-txt和t2-txt中的单词进行计数"><a href="#对t1-txt和t2-txt中的单词进行计数" class="headerlink" title="对t1.txt和t2.txt中的单词进行计数"></a>对t1.txt和t2.txt中的单词进行计数</h2><hr>
<p>先把<code>/output</code>目录删除了：</p>
<pre><code>$HADOOP_PREFIX/bin/hadoop fs -rmr /output
</code></pre><p>将上面的<code>WordCount.java</code>中<code>main()</code>函数中的</p>
<pre><code>FileInputFormat.addInputPath(job, new Path(&quot;/input/t1.txt&quot;));
</code></pre><p>替换为：</p>
<pre><code class="java">FileInputFormat.addInputPath(job, new Path(&quot;/input/t1.txt&quot;));
FileInputFormat.addInputPath(job, new Path(&quot;/input/t2.txt&quot;));
</code></pre>
<p>或者替换为：</p>
<pre><code class="java">FileInputFormat.setInputPaths(job, &quot;/input&quot;);
</code></pre>
<p>MapReduce的结果如下：</p>
<pre><code class="java">zsh &gt;&gt; $HADOOP_PREFIX/bin/hadoop fs -cat /output/part-r-00000
&quot;Sign    1
By    1
GitHub&quot;,    1
GitHub.    1
Sign    1
We    1
account    1
agree    1
and    3
clicking    1
emails    1
for    2
occasionally    1
of    1
our    1
policy.    1
privacy    1
related    1
send    1
service    1
terms    1
to    1
up    2
will    1
you    3
</code></pre>
<p><code>and</code>和<code>you</code>出现的次数变成了3，结果正确。</p>
<p>关于<code>FileInputFormat</code>，具体可查看官方文档：<a href="https://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html" target="_blank" rel="external">Class FileInputFormat<k,v></k,v></a>。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.letiantian.me/2014-11-16-jasper-document-classify-reading-notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Letian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/content/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="樂天笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014-11-16-jasper-document-classify-reading-notes/" itemprop="url">Jasper文本分类系列博客阅读摘录</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-11-16T20:12:08+08:00">
                November 16th 2014
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>2014-11-16</p>
<p>今天阅读了Jasper的系列博客<a href="http://www.blogjava.net/zhenandaci/category/31868.html?Show=All" target="_blank" rel="external">文本分类技术</a>，写得很不错，浅显易懂，于是做了些摘录。</p>
<h2 id="“文本分类入门”相关的几篇博客"><a href="#“文本分类入门”相关的几篇博客" class="headerlink" title="“文本分类入门”相关的几篇博客"></a>“文本分类入门”相关的几篇博客</h2><hr>
<p>文本分类有个重要前提：即只能根据文章的文字内容进行分类，而不应借助诸如文件的编码格式，文章作者，发布日期等信息。</p>
<p>文本分类核心的问题便转化为用哪些特征表示一个文本才能保证有效和快速的分类。</p>
<p>向量模型（俗称的VSM，向量空间模型）成了适合文本分类问题的文档表示模型。在这种模型中，一篇文章被看作特征项集合来看，利用加权特征项构成向量进行文本表示，利用词频信息对文本特征进行加权。它实现起来比较简单，并且分类准确度也高，能够满足一般应用的要求。</p>
<p>而实际上，文本是一种信息载体，其所携带的信息由几部分组成：如组成元素本身的信息（词的信息）、组成元素之间顺序关系带来的信息以及上下文信息（更严格的说，还包括阅读者本身的背景和理解）。对于扩充文档表示模型所包含的信息量，人们也做过有益的尝试，例如被称为LSI（Latent Semantic Index潜在语义索引）的方法，就被实验证明保留了一定的语义信息。</p>
<p>训练阶段要解决的第一个问题，即如何选取那些最具代表性的词汇（更严格的说法应该是，那些最具代表性的特征）。对这个问题的解决，有人叫它特征提取，也有人叫它降维。</p>
<p>特征提取实际上有两大类方法。一类称为<strong>特征选择（Term Selection）</strong>，指的是从原有的特征（那许多有用无用混在一起的词汇）中提取出少量的，具有代表性的特征，但特征的类型没有变化（原来是一堆词，特征提取后仍是一堆词，数量大大减少了而已）。另一类称为<strong>特征抽取（Term Extraction）</strong>的方法则有所不同，它从原有的特征中重构出新的特征（原来是一堆词，重构后变成了别的，例如LSI将其转为矩阵，文档生成模型将其转化为某个概率分布的一些参数），新的特征具有更强的代表性，并耗费更少的计算资源。</p>
<p>对中文文本来说，首先要经历一个分词的过程，就是把连续的文字流切分成一个一个单独的词汇。</p>
<p>中文分词的效果对文本分类系统的表现影响很大，因为在后面的流程中，全都使用预处理之后的文本信息，不再参考原始文本，因此分词的效果不好，等同于引入了错误的训练数据。分词本身也是一个值得大书特书的问题，目前比较常用的方法有词典法，隐马尔科夫模型和新兴的<strong>CRF</strong>方法。</p>
<p>英文只需要通过空格和标点便很容易将一个一个独立的词从原文中区分出来。不过，英文文本还有进一步简化和压缩的空间。我们都知道，英文中同一个词有所谓词形的变化（相对的，词义本身却并没有变），例如名词有单复数的变化，动词有时态的变化，形容词有比较级的变化等等，还包括这些变化形式的某种组合。而正因为词义本身没有变化，仅仅词形不同的词就不应该作为独立的词来存储和和参与分类计算。去除这些词形不同，但词义相同的词，仅保留一个副本的步骤就称为<strong>“词根还原”</strong>。</p>
<p>一般来说类别之间的关系都是可以表示成树形结构，这意味着一个类有多个子类，而一个子类唯一的属于一个父类。这种类别体系很常用，却并不代表它在现实世界中也是符合常识的，举个例子，“临床心理学”这个类别应该即属于“临床医学”的范畴，同时也属于“心理学”，但在分类系统中却不便于使用这样的结构。想象一下，这相当于类别的层次结构是一个有环图，无论遍历还是今后类别的合并，比较，都会带来无数的麻烦。</p>
<h2 id="“SVM”相关的几篇博客"><a href="#“SVM”相关的几篇博客" class="headerlink" title="“SVM”相关的几篇博客"></a>“SVM”相关的几篇博客</h2><hr>
<p>SVM 方法有很坚实的理论基础，SVM 训练的本质是解决一个二次规划问题（Quadruple Programming，指目标函数为二次函数，约束条件为线性约束的最优化问题），得到的是全局最优解，这使它有着其他统计学习技术难以比拟的优越性。SVM 分类器的文本分类效果很好，是最好的分类器之一。同时使用核函数将原始的样本空间向高维空间进行变换，能够解决原始样本线性不可分的问题。其缺点是核函数的选择缺乏指导，难以针对具体问题选择最佳的核函数；另外SVM 训练速度极大地受到训练集规模的影响，计算开销比较大。</p>
<p><a href="http://www.blogjava.net/zhenandaci/archive/2009/03/26/262113.html" target="_blank" rel="external"><strong>将SVM用于多分类的三种方法</strong></a> </p>
<h2 id="“特征选取”相关的几篇博客"><a href="#“特征选取”相关的几篇博客" class="headerlink" title="“特征选取”相关的几篇博客"></a>“特征选取”相关的几篇博客</h2><hr>
<p>为分类文本作处理的特征提取算法也对最终效果有巨大影响，而特征提取算法又分为特征选择和特征抽取两大类，其中特征选择算法有<strong>互信息，文档频率，信息增益，开方检验</strong>等等十数种。</p>
<p>对于开方检验，开方值越大则说明越应该作为特征被选中。</p>
<h2 id="“Java中的字符集编码”相关的几篇博客"><a href="#“Java中的字符集编码”相关的几篇博客" class="headerlink" title="“Java中的字符集编码”相关的几篇博客"></a>“Java中的字符集编码”相关的几篇博客</h2><hr>
<p><strong>面向字符</strong>和<strong>面向字节</strong>中的所谓“面向”什么，是指这些类在处理输入输出的时候，在哪个意义上保持一致。如果面向字节，那么这类工作要保证系统中的文件二进制内容和读入JVM内部的二进制内容要一致。不能变换任何0和1的顺序。因此这是一种非常“忠实于原著”的做法。</p>
<p>这种输入输出方式很适合读入视频文件或者音频文件，或者任何不需要做变换的文件内容。</p>
<p>而面向字符的IO是指希望系统中的文件的字符和读入内存的“字符”（注意和字节的区别）要一致。例如我们的中文版WindowsXP系统上有一个GBK的文本文件，其中有一个“汉”字，这个字的GBK编码是0xBABA（而UTF-16编码是0x6C49），当我们使用面向字符的IO把它读入内存并保存在一个char型变量中时，我希望IO系统不要傻傻的直接把0xBABA放到这个char型变量中，我甚至都不关心这个char型变量具体的二进制内容到底是多少，我只希望这个字符读进来之后仍然是“汉”这个字。</p>
<h2 id="关于网页编码"><a href="#关于网页编码" class="headerlink" title="关于网页编码"></a>关于网页编码</h2><hr>
<p>说到GB2312和GBK就不得不提中文网页的编码。尽管很多新开发的Web系统和新上线的注重国际化的网站都开始使用UTF-8，仍有相当一部分的中文媒体坚持使用GB2312和GBK，例如新浪的页面。其中有两点很值得注意。</p>
<p>第一，html中meta标签的部分，常常可以见到</p>
<pre><code>charset=GB2312
</code></pre><p>这样的写法，很不幸的是，这个“charset”其实是用来指定页面使用的是什么字符集编码，而不是使用什么字符集。例如你见到过有人写“charset=UTF-8”，见到过有人写“charset=ISO-8859-1”，但你见过有人写“charset=Unicode”么？当然没有，因为Unicode是一个字符集，而不是编码。</p>
<p>然而正是charset这个名称误导了很多程序员，真的以为这里要指定的是字符集，也因而使他们进一步的误以为UTF-8和UTF-16是一种字符集！（万恶啊）好在XML中已经做出了修改，这个位置改成了正确的名称：encoding。</p>
<p>第二，页面中说的GB2312，实际上并不真的是GB2312（惊讶么？）。我们来做个实验，例如找一个GB2312中不存在的汉字“亸”（这个字确实不在GB2312中，你可以到GB2312的码表中去找，保证找不到），这个字在GBK中。然后你把它放到一个html页面中，试着在浏览器中打开它，然后选择浏览器的编码为“GB2312”，看到了什么？它完全正常显示！</p>
<p>结论不用我说你也明白了，浏览器实际上使用的是GBK来显示。<br>新浪的页面中也有很多这样的例子，到处都写charset=GB2312，却使用了无数个GB2312中并不存在的字符。这种做法对浏览器显示页面并不成问题，但在需要程序抓取页面并保存的时候带来了麻烦，程序将不能依据页面所“声称”的编码进行读取和保存，而只能尽量猜测正确的编码。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.letiantian.me/2014-11-04-get-max-and-min-values-for-a-given-key/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Letian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/content/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="樂天笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014-11-04-get-max-and-min-values-for-a-given-key/" itemprop="url">如何在mongodb中获取某个键的最大值、最小值</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-11-04T22:59:56+08:00">
                November 4th 2014
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>2014-11-04</p>
<p>最近，遇到这样一个需求，就是计算mongodb中一个集合里某个键的最大最小值，<del>官方没有比较简洁的指令，只能想想其他办法。</del></p>
<p>MongoDB版本是2.4，在数据库s中test集合下，每个文档都是这样的形式（文档中省略了其他的键）：</p>
<pre><code>&gt; db.test.findOne()
{
    &quot;_id&quot; : ObjectId(&quot;5456d5b9cc7a670d39a9ecdb&quot;),
    &quot;vid&quot; : 6096,
}
&gt;
</code></pre><p>我的目标是获取<code>vid</code>的最大值和最小值。</p>
<h3 id="第1种解决办法：使用MapReduce"><a href="#第1种解决办法：使用MapReduce" class="headerlink" title="第1种解决办法：使用MapReduce"></a>第1种解决办法：使用MapReduce</h3><p>我在<a href="/2014-03-22-mapreduce-in-mongodb/">MapReduce in MongoDB</a>介绍过在MongoDB中如何使用mapreduce。官方的一篇文档<a href="http://cookbook.mongodb.org/patterns/finding_max_and_min_values_for_a_key/" target="_blank" rel="external">Finding Max And Min Values for a given Key</a>也介绍了如何在某种情况下如何使用mapreduce获取最大值/最小值。</p>
<p>对这次需求而言，需要使用下面的代码：</p>
<pre><code>var map = function() {
    id = this.vid;
    emit(&quot;vid&quot;,{max:id, min:id})    
}
var reduce = function(key, values) {
    var min = values[0].min;
    var max = values[0].max;
    for ( var i=1; i&lt;values.length; i++ ) {
        if (min &gt; values[i].min) {
            min = values[i].min;
        }   
        if (max &lt; values[i].max) {
            max = values[i].max;
        }   
    }
    return {&quot;min&quot;:min, &quot;max&quot;:max};
}

db.test.mapReduce(
    map,
    reduce,
    {out:{inline:1}}
);
</code></pre><p>执行结果如下：</p>
<pre><code>{
    &quot;results&quot; : [
        {
            &quot;_id&quot; : &quot;vid&quot;,
            &quot;value&quot; : {
                &quot;min&quot; : 1,
                &quot;max&quot; : 6110
            }
        }
    ],
    &quot;timeMillis&quot; : 1078,
    &quot;counts&quot; : {
        &quot;input&quot; : 6110,
        &quot;emit&quot; : 6110,
        &quot;reduce&quot; : 62,
        &quot;output&quot; : 1
    },
    &quot;ok&quot; : 1,
}
</code></pre><p>对于小数据集，速度很快。然后我用了一个大数据集，一个有12000左右个文档的集合，文档数量不算多，但是每个文档很大，最终占用30GB左右的空间，在对<code>vid</code>进行索引的情况下，在10分钟内没有输出结果，我就把它中断了。</p>
<h3 id="第2种解决办法：使用一个文档来记录最大最小值"><a href="#第2种解决办法：使用一个文档来记录最大最小值" class="headerlink" title="第2种解决办法：使用一个文档来记录最大最小值"></a>第2种解决办法：使用一个文档来记录最大最小值</h3><p>在小数据集的情况下，将得到的最小值，最大值放入一个文档里。然后，当插入新的数据时，判断是否更新最小值、最大值。</p>
<h3 id="还有更多方法"><a href="#还有更多方法" class="headerlink" title="还有更多方法"></a>还有更多方法</h3><p>2015-05-23补充。<br>感谢@lhrkkk的提醒，可见自己对mongodb了解的还不够。</p>
<p>假设现在集合link下有以下三个文档：</p>
<pre><code class="json">{ &quot;_id&quot; : NumberLong(22), &quot;status&quot; : 1, &quot;topic_id&quot; : NumberLong(34)}
{ &quot;_id&quot; : NumberLong(23), &quot;status&quot; : 1, &quot;topic_id&quot; : NumberLong(35)}
{ &quot;_id&quot; : NumberLong(24), &quot;status&quot; : 1, &quot;topic_id&quot; : NumberLong(35)}
</code></pre>
<p><strong>使用sort</strong>  </p>
<pre><code>&gt; db.link.find().sort({&quot;topic_id&quot;:1}).limit(1)   // topic_id最小的doc
{ &quot;_id&quot; : NumberLong(22), &quot;status&quot; : 1, &quot;topic_id&quot; : NumberLong(34)}
&gt; db.link.find().sort({&quot;topic_id&quot;:-1}).limit(1)  // topic_id最大的doc
{ &quot;_id&quot; : NumberLong(23), &quot;status&quot; : 1, &quot;topic_id&quot; : NumberLong(35)}
&gt; db.link.find().sort({&quot;topic_id&quot;:-1}).limit(2)  // topic_id最大两个的doc
{ &quot;_id&quot; : NumberLong(23), &quot;status&quot; : 1, &quot;topic_id&quot; : NumberLong(35)}
{ &quot;_id&quot; : NumberLong(24), &quot;status&quot; : 1, &quot;topic_id&quot; : NumberLong(35)}
</code></pre><p><strong>使用aggregate</strong>  </p>
<pre><code>// 获取最小的_id
&gt; db.link.aggregate(
   [
     {
       $group:
         {
           _id: &quot;min_id&quot;,
           min_id: { $min: &quot;$_id&quot; }
         }
     }
   ]
) // Enter

{
    &quot;result&quot; : [
        {
            &quot;_id&quot; : &quot;min_id&quot;,
            &quot;min_id&quot; : NumberLong(22)
        }
    ],
    &quot;ok&quot; : 1
}

// 按照topic_id分组，得到每组中的_id的最小值
&gt; db.link.aggregate(
   [
     {
       $group:
         {
           _id: &quot;$topic_id&quot;,
           min_id: { $min: &quot;$_id&quot; }
         }
     }
   ]
) // Enter

{
    &quot;result&quot; : [
        {
            &quot;_id&quot; : NumberLong(35),
            &quot;min_id&quot; : NumberLong(23)
        },
        {
            &quot;_id&quot; : NumberLong(34),
            &quot;min_id&quot; : NumberLong(22)
        }
    ],
    &quot;ok&quot; : 1
}
</code></pre><p>（ 完 ）</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.letiantian.me/2014-11-04-array-pointer-and-2d-array/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Letian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/content/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="樂天笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014-11-04-array-pointer-and-2d-array/" itemprop="url">数组指针与二维数组</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-11-04T22:32:00+08:00">
                November 4th 2014
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>2014-11-04</p>
<p>今天，偶遇到一C语言面试题，问题是求下面代码的输出：</p>
<pre><code>    int main(void)  
    {  
        char aa[][3] = {&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;};  
        char (*p)[3] = aa;  
        p++;  

        printf(&quot;%c\n&quot;,**p);  
        return 0;  
    }
</code></pre><p>标准答案是<code>d</code>。由于太长时间没用过C语言了，有点懵，就把<a href="/2014-07-24-c-int-pointer/">再学习c指针-int型指针及其他</a>翻出来看了下。</p>
<p>下面一行一行解释下这段代码。</p>
<p>首先：</p>
<pre><code>char aa[][3] = {&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;};
</code></pre><p>定义了一个2行3列的二维数组，我们把它想象成矩阵，第0行存储的是<code>&#39;a&#39;,&#39;b&#39;,&#39;c&#39;</code>，第1行存储的是<code>&#39;d&#39;,&#39;e&#39;,&#39;f&#39;</code>。当然，这六个元素在内存中是依次排列、紧紧挨着的。</p>
<p>然后：</p>
<pre><code>char (*p)[3] = aa;
</code></pre><p>定义了一个名为p的<code>数组指针</code>。<code>数组指针</code>是指该指针指向一个数组，[3]指明被指向的数组最小的大小。另外，有一个<code>指针数组</code>的概念，<code>char *q[3]</code>是一个指针数组，q是一个具有3个元素的数组，每个元素是一个指向char类型变量的指针。</p>
<p><code>p</code>的默认值是<code>aa</code>数组第0行第0列元素的地址，也就是<code>aa</code>的首地址，<code>p++</code>将p向下转移了一行，让p的值变成了<code>aa</code>数组第1行第0列元素的地址，所以：</p>
<pre><code>printf(&quot;%c\n&quot;,**p);
</code></pre><p>输出<code>d</code>。</p>
<p>我在上面代码的基础上加了些冗余的代码，也许更容易理解：</p>
<pre><code># include &lt;stdio.h&gt;
# include &lt;stdlib.h&gt;

/*
 * 
 */
int main(int argc, char** argv) {
    char aa[][3] = {&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;};   // aa[2][3]
    int i,j;
    for(i=0; i&lt;2; i++) {
        for(j=0; j&lt;3; j++) {
            printf(&quot;%c\n&quot;, aa[i][j]);
        }
    }

    char (*p)[3] = aa;  // 数组指针

    printf(&quot;---\n&quot;);
    printf(&quot;%p\n&quot;, p);  //输出地址值，每次运行，结果可能不同
    printf(&quot;%p\n&quot;, aa); // 和上一语句输出相同
    printf(&quot;---\n&quot;);

    printf(&quot;%c\n&quot;, (*p)[0]);  //a
    printf(&quot;%c\n&quot;, (*p)[1]);  //b
    printf(&quot;%c\n&quot;, (*p)[2]);  //c
    printf(&quot;%c\n&quot;, (*p)[3]);  //d
    printf(&quot;%c\n&quot;, (*p)[4]);  //e

    printf(&quot;---\n&quot;);

    printf(&quot;%c\n&quot;,**p);  //a
    printf(&quot;%c\n&quot;,**(p+1));  //d
    printf(&quot;%c\n&quot;,*(*(p+1)+1));  //aa[1][1]-&gt; e

    p++;  
    printf(&quot;%c\n&quot;,**p);  //aa[1][0] -&gt; d 
    return (EXIT_SUCCESS);
}
</code></pre><p>运行结果：</p>
<pre><code>a
b
c
d
e
f
---
0x7fffa0bd1a70
0x7fffa0bd1a70
---
a
b
c
d
e
---
a
d
e
d
</code></pre><p>（ 完 ）</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/7/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" href="/page/9/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/content/images/avatar.jpg"
               alt="Letian" />
          <p class="site-author-name" itemprop="name">Letian</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">209</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">43</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/letiantian" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Letian</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.1"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  
    
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "0b5e0b416d0b4d9a845ed9b2e72ddc70",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
  










  





  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['[latex]','[/latex]'], ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        },
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  


  <!-- highlight -->
  <script src="/highlight/highlight.min.js"></script>
  <link rel="stylesheet" href="/highlight/styles/github.css">

  <script>
    // 高亮
    hljs.initHighlightingOnLoad();
  </script>

</body>
</html>
